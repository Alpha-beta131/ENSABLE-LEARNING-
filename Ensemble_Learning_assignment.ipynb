{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "THEORATICAL QUESTION"
      ],
      "metadata": {
        "id": "BeqKJu9xW7zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Can we use Bagging for regression problems?\n",
        "-  Ans- Yes, Bagging can be used for regression problems. Bagging (Bootstrap Aggregating) is a general ensemble method that works for both classification and regression tasks. The principle is the same in both cases:\n",
        "\n",
        "Bootstrap sampling: Multiple subsets of the training data are created by random sampling with replacement.\n",
        "\n",
        "Train base models: A separate base model (often a decision tree) is trained on each bootstrap sample.\n",
        "\n",
        "Aggregate predictions:\n",
        "\n",
        "For classification, the predictions are combined using majority voting."
      ],
      "metadata": {
        "id": "7aC1iDwVXFxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between multiple model training and single model training?\n",
        "-  Ans- The difference between multiple model training and single model training lies in how predictions are made, how variance and bias are handled, and how robust the model is. Let‚Äôs break it down clearly:\n",
        "\n",
        "1. Single Model Training\n",
        "\n",
        "Definition: You train one model on the entire dataset.\n",
        "\n",
        "Workflow:\n",
        "\n",
        "Collect data ‚Üí 2. Train a model (e.g., Decision Tree, Linear Regression) ‚Üí 3. Predict on new data.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Simpler and faster to train.\n",
        "\n",
        "The performance depends entirely on the chosen model and its hyperparameters.\n",
        "\n",
        "Sensitive to noise and variance (especially for high-variance models like Decision Trees).\n",
        "\n",
        "Can overfit if the model is too complex or underfit if too simple.\n",
        "\n",
        "Example: Training a single decision tree on a dataset.\n",
        "\n",
        "2. Multiple Model Training (Ensemble Learning)\n",
        "\n",
        "Definition: You train multiple models (often called base learners) and combine their predictions.\n",
        "\n",
        "Workflow:\n",
        "\n",
        "Collect data ‚Üí 2. Train multiple models (possibly on different subsets of data or with different parameters) ‚Üí 3. Combine predictions via averaging, voting, or stacking.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "More robust and stable; reduces variance and/or bias depending on the ensemble technique.\n",
        "\n",
        "Can handle noisy datasets better.\n",
        "\n",
        "Slightly more complex and computationally intensive.\n",
        "\n",
        "Helps improve generalization and accuracy.\n",
        "\n",
        "Techniques:\n",
        "\n",
        "Bagging: Multiple models trained on bootstrap samples; predictions averaged (regression) or voted (classification).\n",
        "\n",
        "Boosting: Models trained sequentially to correct previous errors.\n",
        "\n",
        "Random Forest: Bagging applied to decision trees with feature randomness.\n",
        "\n",
        "Example: Training 100 decision trees on different bootstrap samples and averaging their predictions ‚Üí Random Forest."
      ],
      "metadata": {
        "id": "Z9dmg-hjXU5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Explain the concept of feature randomness in Random Forest.\n",
        "-  Ans- The concept of feature randomness in Random Forest is one of the key ideas that makes Random Forest powerful and different from simple Bagging with decision trees. Let‚Äôs break it down carefully:\n",
        "\n",
        "1. What is Random Forest?\n",
        "\n",
        "Random Forest is an ensemble method that builds multiple decision trees and combines their predictions (majority voting for classification, averaging for regression). It reduces variance and improves accuracy compared to a single decision tree.\n",
        "\n",
        "2. The Problem with Regular Bagging\n",
        "\n",
        "In Bagging, each tree is trained on a bootstrap sample of the data.\n",
        "\n",
        "But if one feature is very strong (highly predictive), most trees will use that same feature at the top split.\n",
        "\n",
        "This causes correlation between trees, which reduces the benefit of averaging‚Äîthey all ‚Äúthink‚Äù the same way.\n",
        "\n",
        "3. Feature Randomness (Random Subspace Method)\n",
        "\n",
        "Random Forest solves this by introducing feature randomness:\n",
        "\n",
        "At each split in a tree, instead of considering all features, the algorithm randomly selects a subset of features.\n",
        "\n",
        "The tree then chooses the best split among this subset.\n",
        "\n",
        "Notation:\n",
        "\n",
        "Suppose we have m total features.\n",
        "\n",
        "For each split, we randomly pick k features (k < m).\n",
        "\n",
        "k is often set to:\n",
        "\n",
        "Classification: k = sqrt(m)\n",
        "\n",
        "Regression: k = m / 3"
      ],
      "metadata": {
        "id": "-r86C9D5XoC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is OOB (Out-of-Bag) Score?\n",
        "-  Ans- The OOB (Out-of-Bag) score is a way to evaluate the performance of ensemble models like Random Forest without using a separate validation set. It‚Äôs a built-in, efficient method for measuring accuracy or error. Let‚Äôs break it down:\n",
        "\n",
        "1. The Idea Behind OOB\n",
        "\n",
        "Random Forest uses bootstrap sampling: each tree is trained on a random sample of the data with replacement.\n",
        "\n",
        "This means that, on average, about 63% of the data is used to train a given tree, and the remaining 37% is left out.\n",
        "\n",
        "The samples not used to train a tree are called Out-of-Bag (OOB) samples."
      ],
      "metadata": {
        "id": "ufmmd3-NX1eR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How can you measure the importance of features in a Random Forest model?\n",
        "-  Ans- In a Random Forest model, feature importance tells us which features contribute the most to the model‚Äôs predictions. Random Forest has built-in ways to measure this. Let‚Äôs break it down clearly.\n",
        "\n",
        "1. Concept of Feature Importance\n",
        "\n",
        "Random Forest builds many decision trees. Each tree splits nodes based on features that reduce impurity the most (e.g., Gini impurity for classification, variance for regression).\n",
        "\n",
        "Features that are used often at the top of trees and result in large reductions in impurity are more important.\n",
        "\n",
        "Feature importance is a score for each feature indicating its relative contribution to the prediction.\n",
        "\n",
        "2. How Feature Importance is Calculated\n",
        "\n",
        "Two main ways:\n",
        "\n",
        "A. Mean Decrease in Impurity (MDI)\n",
        "\n",
        "Each time a feature is used to split a node:\n",
        "\n",
        "Calculate the reduction in impurity caused by that split.\n",
        "\n",
        "Weight it by the number of samples reaching that node.\n",
        "\n",
        "Sum this over all trees in the forest for each feature.\n",
        "\n",
        "Normalize to get a score between 0 and 1.\n",
        "\n",
        "Higher score ‚Üí more important feature.\n",
        "\n",
        "B. Mean Decrease in Accuracy (Permutation Importance)\n",
        "\n",
        "For each feature:\n",
        "\n",
        "Randomly shuffle the feature values in the test set (break the relationship with target).\n",
        "\n",
        "Measure how much the model‚Äôs accuracy drops.\n",
        "\n",
        "Larger drop ‚Üí the feature is more important."
      ],
      "metadata": {
        "id": "fUwAbi1RYDCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the working principle of a Bagging Classifier.\n",
        "-  Ans- 1. What is Bagging?\n",
        "\n",
        "Bagging stands for Bootstrap Aggregating. It is an ensemble learning technique used to improve the accuracy and stability of machine learning models, mainly high-variance models like decision trees.\n",
        "\n",
        "Idea: Train multiple models on different subsets of data and combine their predictions to reduce variance and prevent overfitting.\n",
        "\n",
        "2. Steps in Bagging Classifier\n",
        "Step 1: Bootstrap Sampling\n",
        "\n",
        "From the original training dataset with N samples, create M new datasets by random sampling with replacement.\n",
        "\n",
        "Each dataset is called a bootstrap sample.\n",
        "\n",
        "Because sampling is with replacement, some samples appear multiple times, and some may not appear in a particular sample.\n",
        "\n",
        "Step 2: Train Base Models\n",
        "\n",
        "Train a base classifier (commonly a decision tree) on each bootstrap sample.\n",
        "\n",
        "Each model is trained independently, so errors or biases of one model do not affect others.\n",
        "\n",
        "Step 3: Aggregate Predictions\n",
        "\n",
        "For classification tasks:\n",
        "\n",
        "Each model predicts a class.\n",
        "\n",
        "Final prediction is made by majority voting.\n",
        "\n",
        "Example: If 10 trees predict [Class A, Class B, Class A, ‚Ä¶], the class predicted by most trees becomes the final output.\n",
        "\n",
        "For regression tasks:\n",
        "\n",
        "Predictions are averaged instead of voting.\n",
        "\n",
        "3. Key Advantages\n",
        "\n",
        "Reduces Variance: By averaging predictions of multiple models, the ensemble is less sensitive to noise in the training data.\n",
        "\n",
        "Prevents Overfitting: Single trees may overfit, but bagging reduces this effect.\n",
        "\n",
        "Works well with unstable models: Models like decision trees benefit the most because small changes in data can drastically change the tree. Bagging stabilizes predictions."
      ],
      "metadata": {
        "id": "ga_UL_kaYT4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you evaluate a Bagging Classifier‚Äôs performance?\n",
        "-  Ans- Evaluating a Bagging Classifier is very similar to evaluating any classification model, but there are a few ensemble-specific considerations. Let‚Äôs go step by step.\n",
        "\n",
        "1. Standard Evaluation Metrics for Classification\n",
        "\n",
        "You can use the same metrics you would for any classifier:\n",
        "\n",
        "Metric\tDescription\n",
        "Accuracy\tFraction of correctly predicted samples:\n",
        "Accuracy\n",
        "=\n",
        "Correct Predictions\n",
        "Total Samples\n",
        "Accuracy=\n",
        "Total Samples\n",
        "Correct Predictions\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Precision\tTrue positives / Predicted positives. Important when false positives are costly.\n",
        "Recall (Sensitivity)\tTrue positives / Actual positives. Important when missing positives is costly.\n",
        "F1-Score\tHarmonic mean of precision and recall. Balances precision and recall.\n",
        "Confusion Matrix\tShows counts of True Positive, True Negative, False Positive, False Negative.\n",
        "ROC-AUC\tMeasures classifier‚Äôs ability to distinguish between classes (especially for binary classification).\n",
        "2. Using Out-of-Bag (OOB) Score\n",
        "\n",
        "Bagging has a built-in evaluation method called OOB score:\n",
        "\n",
        "Each tree is trained on a bootstrap sample, leaving some data points out (OOB samples).\n",
        "\n",
        "For each OOB sample, predict its class using the trees that did not see it during training.\n",
        "\n",
        "Compare the predicted classes with true classes to compute OOB accuracy.\n",
        "\n",
        "Advantage: No need for a separate validation set, which is useful for small datasets."
      ],
      "metadata": {
        "id": "z5PYvzOZYgR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. How does a Bagging Regressor work?\n",
        "-  Ans- A Bagging Regressor is the regression version of Bagging (Bootstrap Aggregating). It works very similarly to a Bagging Classifier but is used for predicting continuous values instead of class labels. Let‚Äôs break it down step by step.\n",
        "\n",
        "1. Basic Idea\n",
        "\n",
        "Bagging Regressor combines predictions from multiple regressors trained on different random subsets of the training data.\n",
        "\n",
        "Purpose: Reduce variance and improve accuracy of unstable regressors (like Decision Trees).\n",
        "\n",
        "2. Working Steps\n",
        "Step 1: Bootstrap Sampling\n",
        "\n",
        "From the original dataset with\n",
        "ùëÅ\n",
        "N samples, generate multiple bootstrap samples (random sampling with replacement).\n",
        "\n",
        "Each sample has roughly 63% of unique data points; the remaining 37% are ‚Äúout-of-bag‚Äù for that sample.\n",
        "\n",
        "Step 2: Train Base Regressors\n",
        "\n",
        "Train a base regression model (often a Decision Tree Regressor) on each bootstrap sample.\n",
        "\n",
        "Each model is trained independently, so errors in one tree do not propagate.\n",
        "\n",
        "Step 3: Aggregate Predictions\n",
        "\n",
        "For a new input, each trained model predicts a numerical value.\n",
        "\n",
        "Final prediction is obtained by averaging all predictions:\n",
        "\n",
        "ùë¶\n",
        "^\n",
        "=\n",
        "1\n",
        "ùëÄ\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëÄ\n",
        "ùë¶\n",
        "^\n",
        "ùëñ\n",
        "y\n",
        "^\n",
        "\t‚Äã\n",
        "\n",
        "=\n",
        "M\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "i=1\n",
        "‚àë\n",
        "M\n",
        "\t‚Äã\n",
        "\n",
        "y\n",
        "^\n",
        "\t‚Äã\n",
        "\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëÄ\n",
        "M = number of regressors (trees)\n",
        "\n",
        "ùë¶\n",
        "^\n",
        "ùëñ\n",
        "y\n",
        "^\n",
        "\t‚Äã\n",
        "\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        " = prediction from the i-th regressor\n",
        "\n",
        "3. Key Points\n",
        "\n",
        "Bagging reduces variance: the average of multiple models is more stable than a single model.\n",
        "\n",
        "Works best with high-variance base models (like decision trees).\n",
        "\n",
        "Each regressor sees slightly different data ‚Üí diverse predictions ‚Üí averaging improves accuracy."
      ],
      "metadata": {
        "id": "yl6l2HOlZC_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main advantage of ensemble techniques?\n",
        "-  Ans- The main advantage of ensemble techniques is that they combine multiple models to produce more accurate, stable, and robust predictions than any single model alone.\n",
        "\n",
        "Here‚Äôs a detailed explanation:\n",
        "\n",
        "1. Reduce Variance, Bias, or Both\n",
        "\n",
        "Ensemble methods improve model performance by addressing errors that single models make:\n",
        "\n",
        "Error Type\tEnsemble Effect\n",
        "High variance (e.g., Decision Trees)\tBagging ensembles reduce overfitting by averaging predictions (stabilizes predictions).\n",
        "High bias (e.g., weak learners)\tBoosting ensembles reduce bias by sequentially focusing on mistakes of previous models.\n",
        "Both variance & bias\tSome ensemble methods (e.g., Stacking) can reduce both, improving overall accuracy.\n",
        "2. Improve Accuracy\n",
        "\n",
        "By combining predictions of multiple models, ensemble techniques generally achieve higher accuracy than a single model.\n",
        "\n",
        "‚ÄúWisdom of the crowd‚Äù effect: even if some models are weak or make mistakes, their errors cancel out, giving better overall predictions.\n",
        "\n",
        "3. Increase Robustness\n",
        "\n",
        "Ensemble models are less sensitive to noise and outliers than individual models.\n",
        "\n",
        "They are more stable: training the ensemble on slightly different data produces similar results, unlike a single high-variance model.\n",
        "\n",
        "4. Handle Complex Problems\n",
        "\n",
        "Some problems are too complex for a single model to capture.\n",
        "\n",
        "Ensembles can combine different types of models to exploit their strengths (e.g., stacking linear models + trees).\n",
        "\n",
        "5. Examples\n",
        "\n",
        "Bagging (Random Forest): reduces variance of decision trees.\n",
        "\n",
        "Boosting (AdaBoost, Gradient Boosting): reduces bias of weak learners.\n",
        "\n",
        "Stacking: combines different models for better generalization."
      ],
      "metadata": {
        "id": "CNPiwON9ZyG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the main challenge of ensemble methods?\n",
        "-  Ans- The main challenge of ensemble methods is that they increase complexity and computational cost compared to single models, which can make them harder to train, interpret, and deploy. Let‚Äôs break it down clearly:\n",
        "\n",
        "1. Increased Computational Cost\n",
        "\n",
        "Ensemble methods involve training multiple models instead of one.\n",
        "\n",
        "More memory and processing power are required, especially for large datasets or complex base models.\n",
        "\n",
        "Example: Random Forest with 500 trees takes much longer to train than a single Decision Tree.\n",
        "\n",
        "2. Reduced Interpretability\n",
        "\n",
        "Single models (like a decision tree or linear regression) are easy to understand and explain.\n",
        "\n",
        "Ensembles combine many models, so understanding why a prediction was made becomes harder.\n",
        "\n",
        "This is a problem in high-stakes applications like finance or healthcare, where model explainability is important.\n",
        "\n",
        "3. Risk of Overfitting (Sometimes)\n",
        "\n",
        "While Bagging reduces overfitting, some ensemble methods like Boosting can overfit if:\n",
        "\n",
        "The number of base learners is too large.\n",
        "\n",
        "The learning rate is too high.\n",
        "\n",
        "Careful hyperparameter tuning is needed.\n",
        "\n",
        "4. More Complex Deployment\n",
        "\n",
        "Deploying an ensemble requires:\n",
        "\n",
        "Storing multiple models.\n",
        "\n",
        "Performing multiple predictions for each input.\n",
        "\n",
        "This can increase latency and resource requirements in real-time systems.\n",
        "\n",
        "5. Hyperparameter Tuning\n",
        "\n",
        "Ensembles have more hyperparameters:\n",
        "\n",
        "Number of base models (trees, estimators)\n",
        "\n",
        "Learning rate (for boosting)\n",
        "\n",
        "Depth of trees, etc.\n",
        "\n",
        "Proper tuning is critical for good performance, which adds complexity."
      ],
      "metadata": {
        "id": "TzoEGMweaBqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Explain the key idea behind ensemble techniques.\n",
        "-  Ans- The key idea behind ensemble techniques is simple but powerful:\n",
        "\n",
        "‚ÄúCombine multiple models to make a single, stronger model that is more accurate, robust, and stable than any individual model.‚Äù\n",
        "\n",
        "Let‚Äôs break this down in detail.\n",
        "\n",
        "1. Motivation\n",
        "\n",
        "A single model may have limitations:\n",
        "\n",
        "High variance ‚Üí overfits training data (e.g., a deep Decision Tree)\n",
        "\n",
        "High bias ‚Üí underfits and misses patterns (e.g., linear regression on nonlinear data)\n",
        "\n",
        "Individual models may make different mistakes on the same data.\n",
        "\n",
        "By combining them, we can reduce errors and improve generalization.\n",
        "\n",
        "2. How Ensemble Works\n",
        "\n",
        "Multiple Models (Base Learners): Train several models on the same dataset or slightly modified datasets.\n",
        "\n",
        "Diversity: Ensure models are different enough so they don‚Äôt all make the same mistakes.\n",
        "\n",
        "Aggregation: Combine predictions from all models to produce a final prediction:\n",
        "\n",
        "Classification ‚Üí majority voting\n",
        "\n",
        "Regression ‚Üí averaging\n",
        "\n",
        "3. Core Principles\n",
        "\n",
        "Wisdom of the Crowd: Just like asking multiple people to make a decision usually gives a better answer than asking one person.\n",
        "\n",
        "Error Reduction:\n",
        "\n",
        "Variance reduction: Bagging ensembles stabilize predictions by averaging many high-variance models.\n",
        "\n",
        "Bias reduction: Boosting ensembles sequentially correct mistakes, reducing bias.\n",
        "\n",
        "Robustness: The final ensemble is less sensitive to noise or outliers than individual models."
      ],
      "metadata": {
        "id": "dFTntYtlaLxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is a Random Forest Classifier?\n",
        "-  Ans- A Random Forest Classifier is an ensemble learning method used for classification tasks. It builds on the idea of bagging and decision trees to create a robust and accurate model. Here‚Äôs a clear breakdown:\n",
        "\n",
        "1. Basic Idea\n",
        "\n",
        "Random Forest creates multiple decision trees during training.\n",
        "\n",
        "Each tree is trained on a random subset of the data (bootstrap sample) and a random subset of features.\n",
        "\n",
        "The final prediction is made by majority voting among all trees.\n",
        "\n",
        "2. How It Works\n",
        "Step 1: Bootstrap Sampling\n",
        "\n",
        "Randomly select samples with replacement from the training data to create multiple datasets.\n",
        "\n",
        "Each tree gets its own dataset (some data points may appear more than once; some may not appear at all).\n",
        "\n",
        "Step 2: Random Feature Selection\n",
        "\n",
        "At each split in a tree, instead of considering all features, a random subset of features is selected.\n",
        "\n",
        "This introduces feature randomness, reducing correlation between trees and improving diversity.\n",
        "\n",
        "Step 3: Train Decision Trees\n",
        "\n",
        "Each tree is trained independently using its bootstrap sample and random features.\n",
        "\n",
        "Trees are deep and unpruned, which makes them high-variance but powerful individually.\n",
        "\n",
        "Step 4: Aggregate Predictions\n",
        "\n",
        "For classification, each tree votes for a class.\n",
        "\n",
        "The class with the most votes becomes the final predicted class.\n",
        "\n",
        "3. Advantages\n",
        "\n",
        "Reduces overfitting compared to a single decision tree.\n",
        "\n",
        "Handles high-dimensional data and large datasets well.\n",
        "\n",
        "Provides feature importance scores to understand which features matter most.\n",
        "\n",
        "Robust to noise and outliers."
      ],
      "metadata": {
        "id": "Wuq-wYhUaZpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are the main types of ensemble techniques?\n",
        "-  Ans- The main types of ensemble techniques can be classified based on how multiple models are combined. There are three widely recognized categories:\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Idea: Reduce variance by training multiple models on different random subsets of the data and aggregating their predictions.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Create multiple bootstrap samples from the training data.\n",
        "\n",
        "Train a separate base model (e.g., decision tree) on each sample.\n",
        "\n",
        "Aggregate predictions:\n",
        "\n",
        "Classification ‚Üí majority voting\n",
        "\n",
        "Regression ‚Üí averaging\n",
        "\n",
        "Key Example:\n",
        "\n",
        "Random Forest (Bagging + feature randomness for trees)\n",
        "\n",
        "When to use:\n",
        "\n",
        "High-variance models prone to overfitting (like decision trees).\n",
        "\n",
        "2. Boosting\n",
        "\n",
        "Idea: Reduce bias by training models sequentially, where each model tries to correct the mistakes of the previous ones.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Train the first weak learner (usually a shallow tree).\n",
        "\n",
        "Identify samples that were predicted incorrectly.\n",
        "\n",
        "Train the next learner to focus more on difficult samples.\n",
        "\n",
        "Combine predictions of all learners using weighted voting or summation.\n",
        "\n",
        "Key Examples:\n",
        "\n",
        "AdaBoost\n",
        "\n",
        "Gradient Boosting\n",
        "\n",
        "XGBoost, LightGBM, CatBoost\n",
        "\n",
        "When to use:\n",
        "\n",
        "To improve weak learners and reduce underfitting.\n",
        "\n",
        "3. Stacking (Stacked Generalization)\n",
        "\n",
        "Idea: Combine different types of models using a meta-learner to make a final prediction.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Train multiple base models (can be different algorithms) on the training data.\n",
        "\n",
        "Use their predictions as inputs/features to a meta-model.\n",
        "\n",
        "Meta-model learns how to best combine base model predictions for the final output.\n",
        "\n",
        "Key Examples:\n",
        "\n",
        "Base models: Logistic Regression, Decision Tree, KNN\n",
        "\n",
        "Meta-model: Linear Regression, Random Forest\n",
        "\n",
        "When to use:\n",
        "\n",
        "To leverage the strengths of diverse models for better performance.\n",
        "\n",
        "Other Variants\n",
        "\n",
        "Voting Ensemble: Simple aggregation of predictions from multiple models (hard or soft voting).\n",
        "\n",
        "Blending: Similar to stacking but uses a holdout validation set instead of cross-validation."
      ],
      "metadata": {
        "id": "z56QsPGranUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is ensemble learning in machine learning?\n",
        "-  Ans- The main types of ensemble techniques can be classified based on how multiple models are combined. There are three widely recognized categories:\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Idea: Reduce variance by training multiple models on different random subsets of the data and aggregating their predictions.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Create multiple bootstrap samples from the training data.\n",
        "\n",
        "Train a separate base model (e.g., decision tree) on each sample.\n",
        "\n",
        "Aggregate predictions:\n",
        "\n",
        "Classification ‚Üí majority voting\n",
        "\n",
        "Regression ‚Üí averaging\n",
        "\n",
        "Key Example:\n",
        "\n",
        "Random Forest (Bagging + feature randomness for trees)\n",
        "\n",
        "When to use:\n",
        "\n",
        "High-variance models prone to overfitting (like decision trees).\n",
        "\n",
        "2. Boosting\n",
        "\n",
        "Idea: Reduce bias by training models sequentially, where each model tries to correct the mistakes of the previous ones.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Train the first weak learner (usually a shallow tree).\n",
        "\n",
        "Identify samples that were predicted incorrectly.\n",
        "\n",
        "Train the next learner to focus more on difficult samples.\n",
        "\n",
        "Combine predictions of all learners using weighted voting or summation.\n",
        "\n",
        "Key Examples:\n",
        "\n",
        "AdaBoost\n",
        "\n",
        "Gradient Boosting\n",
        "\n",
        "XGBoost, LightGBM, CatBoost\n",
        "\n",
        "When to use:\n",
        "\n",
        "To improve weak learners and reduce underfitting.\n",
        "\n",
        "3. Stacking (Stacked Generalization)\n",
        "\n",
        "Idea: Combine different types of models using a meta-learner to make a final prediction.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Train multiple base models (can be different algorithms) on the training data.\n",
        "\n",
        "Use their predictions as inputs/features to a meta-model.\n",
        "\n",
        "Meta-model learns how to best combine base model predictions for the final output.\n",
        "\n",
        "Key Examples:\n",
        "\n",
        "Base models: Logistic Regression, Decision Tree, KNN\n",
        "\n",
        "Meta-model: Linear Regression, Random Forest\n",
        "\n",
        "When to use:\n",
        "\n",
        "To leverage the strengths of diverse models for better performance.\n",
        "\n",
        "Other Variants\n",
        "\n",
        "Voting Ensemble: Simple aggregation of predictions from multiple models (hard or soft voting).\n",
        "\n",
        "Blending: Similar to stacking but uses a holdout validation set instead of cross-validation."
      ],
      "metadata": {
        "id": "W32Bx1_ba4kX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should we avoid using ensemble methods?\n",
        "-  Ans- We should avoid using ensemble methods in certain situations where their advantages don‚Äôt outweigh the downsides. Here‚Äôs a detailed breakdown:\n",
        "\n",
        "1. When Simplicity and Interpretability Matter\n",
        "\n",
        "Ensemble models (Random Forest, Boosting, Stacking) are complex and harder to interpret than single models.\n",
        "\n",
        "Avoid if:\n",
        "\n",
        "You need a transparent model for decision-making (e.g., in finance, healthcare, or legal applications).\n",
        "\n",
        "Explaining predictions to stakeholders is important.\n",
        "\n",
        "2. When Computational Resources Are Limited\n",
        "\n",
        "Ensembles require more training time, memory, and CPU/GPU power than a single model.\n",
        "\n",
        "Avoid if:\n",
        "\n",
        "You need fast training or prediction in real-time applications.\n",
        "\n",
        "Your hardware cannot handle multiple models or large datasets.\n",
        "\n",
        "3. When Dataset is Small\n",
        "\n",
        "Ensembles perform best when there‚Äôs enough data to train multiple models effectively.\n",
        "\n",
        "With very small datasets:\n",
        "\n",
        "Base models may be weak and correlated.\n",
        "\n",
        "Ensembles may not improve accuracy and can overfit.\n",
        "\n",
        "4. When a Single Model Already Performs Well\n",
        "\n",
        "If a simple model (like linear regression or a single decision tree) already achieves high accuracy:\n",
        "\n",
        "Ensembles may provide minimal improvement.\n",
        "\n",
        "Added complexity may not justify the marginal gain.\n",
        "\n",
        "5. When Low Latency Is Required\n",
        "\n",
        "Ensembles combine multiple models for prediction.\n",
        "\n",
        "Avoid if:\n",
        "\n",
        "Your application requires instant predictions, e.g., mobile apps, embedded systems, or real-time control systems.\n",
        "\n",
        "6. When Model Maintenance is Important\n",
        "\n",
        "Ensembles are harder to maintain:\n",
        "\n",
        "Multiple base models must be updated if data changes.\n",
        "\n",
        "Hyperparameter tuning is more complex."
      ],
      "metadata": {
        "id": "-cb4EyP6bFuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  How does Bagging help in reducing overfitting?\n",
        "-  Ans- Bagging (Bootstrap Aggregating) helps in reducing overfitting primarily by reducing the variance of high-variance models, such as decision trees. Let‚Äôs break this down step by step.\n",
        "\n",
        "1. Understanding Overfitting\n",
        "\n",
        "Overfitting happens when a model learns the noise in the training data instead of just the underlying pattern.\n",
        "\n",
        "High-variance models (like deep decision trees) are prone to overfitting because small changes in training data can lead to very different predictions.\n",
        "\n",
        "2. How Bagging Works\n",
        "\n",
        "Bootstrap Sampling:\n",
        "\n",
        "Bagging generates multiple random subsets of the training data by sampling with replacement.\n",
        "\n",
        "Each base model is trained on a different subset.\n",
        "\n",
        "Train Multiple Models:\n",
        "\n",
        "A separate model (usually a decision tree) is trained on each subset independently.\n",
        "\n",
        "Aggregate Predictions:\n",
        "\n",
        "For classification ‚Üí use majority voting.\n",
        "\n",
        "For regression ‚Üí use averaging.\n",
        "\n",
        "3. Why Bagging Reduces Overfitting\n",
        "\n",
        "Averaging Multiple Predictions Reduces Variance:\n",
        "\n",
        "Individual trees may overfit their subset, but averaging their predictions cancels out individual errors.\n",
        "\n",
        "Example: If one tree predicts 10, another 12, and another 9 ‚Üí the average (‚âà10.3) is closer to the true value.\n",
        "\n",
        "Diversifies Errors Across Trees:\n",
        "\n",
        "Since each tree sees a different dataset, their overfitting patterns are uncorrelated.\n",
        "\n",
        "Aggregation smooths out these errors, making the ensemble more generalizable.\n",
        "\n",
        "Stabilizes Predictions:\n",
        "\n",
        "A single deep tree may change drastically if the data changes slightly.\n",
        "\n",
        "Bagging makes the final model less sensitive to noise, reducing the chance of overfitting.\n",
        "\n",
        "4. Intuition\n",
        "\n",
        "Think of asking 10 people to predict a value:\n",
        "\n",
        "Each person might have a slightly biased or overfitted opinion.\n",
        "\n",
        "Taking the average or majority vote gives a more accurate and stable result than trusting just one person."
      ],
      "metadata": {
        "id": "aKDPXiM_bXAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Why is Random Forest better than a single Decision Tree?\n",
        "-  Ans- Random Forest is generally better than a single Decision Tree because it addresses the main weaknesses of individual trees‚Äîoverfitting, high variance, and instability. Here‚Äôs a detailed explanation:\n",
        "\n",
        "1. Single Decision Tree Limitations\n",
        "\n",
        "A single tree tends to overfit the training data, especially if it is deep.\n",
        "\n",
        "Predictions are highly sensitive to small changes in the data (unstable).\n",
        "\n",
        "Cannot generalize well for complex datasets with noisy features.\n",
        "\n",
        "2. How Random Forest Improves Upon This\n",
        "\n",
        "Random Forest is an ensemble of decision trees, and it introduces two main ideas:\n",
        "\n",
        "A. Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Each tree is trained on a random subset of the data (with replacement).\n",
        "\n",
        "Reduces variance because averaging predictions across multiple trees smooths out individual overfitting.\n",
        "\n",
        "B. Random Feature Selection\n",
        "\n",
        "At each split in a tree, only a random subset of features is considered.\n",
        "\n",
        "Reduces correlation between trees, making the ensemble more diverse.\n",
        "\n",
        "Ensures that even weaker features can sometimes contribute, improving robustness."
      ],
      "metadata": {
        "id": "MlnmBqCmbtCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the role of bootstrap sampling in Bagging?\n",
        "-  Ans- The role of bootstrap sampling in Bagging is central‚Äîit‚Äôs what allows Bagging to reduce variance and improve model stability. Let‚Äôs go step by step:\n",
        "\n",
        "1. What is Bootstrap Sampling?\n",
        "\n",
        "Bootstrap sampling means randomly sampling the training data with replacement to create multiple new datasets.\n",
        "\n",
        "Each new dataset (called a bootstrap sample) has the same number of samples as the original dataset.\n",
        "\n",
        "Because sampling is with replacement, some data points appear multiple times, while others may not appear at all in a given sample.\n",
        "\n",
        "Roughly 63% of original samples appear in each bootstrap sample, leaving 37% as ‚Äúout-of-bag‚Äù (OOB) samples.\n",
        "\n",
        "2. How Bootstrap Sampling Works in Bagging\n",
        "\n",
        "Generate M bootstrap samples from the original dataset.\n",
        "\n",
        "Train a base model (like a decision tree) on each bootstrap sample independently.\n",
        "\n",
        "Combine predictions from all models:\n",
        "\n",
        "Classification: majority voting\n",
        "\n",
        "Regression: averaging\n",
        "\n",
        "3. Why Bootstrap Sampling is Important\n",
        "A. Introduces Diversity\n",
        "\n",
        "Each model sees a slightly different dataset ‚Üí trees are not identical.\n",
        "\n",
        "Reduces correlation between base models, which makes aggregation more effective.\n",
        "\n",
        "B. Reduces Overfitting\n",
        "\n",
        "Individual models may overfit their bootstrap sample, but averaging their predictions cancels out errors.\n",
        "\n",
        "Helps Bagging stabilize predictions and generalize better to unseen data.\n",
        "\n",
        "C. Enables OOB Evaluation\n",
        "\n",
        "The 37% of samples not in a bootstrap sample can be used as a validation set for that tree.\n",
        "\n",
        "Provides an internal estimate of model performance without needing a separate test set.\n",
        "\n",
        "4. Intuition\n",
        "\n",
        "Think of it like asking 10 slightly different groups of people to vote on the same question:\n",
        "\n",
        "Each group is slightly different because of sampling.\n",
        "\n",
        "Individual mistakes cancel out ‚Üí final answer is more reliable.\n",
        "\n",
        "5. Example in Python\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Bagging Classifier with bootstrap enabled (default)\n",
        "bag_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=10,\n",
        "    bootstrap=True,   # Enables bootstrap sampling\n",
        "    oob_score=True,   # Use OOB samples for internal evaluation\n",
        "    random_state=42\n",
        ")\n",
        "bag_clf.fit(X, y)\n",
        "\n",
        "print(\"OOB Score:\", bag_clf.oob_score_)\n",
        "Summary\n",
        "\n",
        "Bootstrap sampling creates diverse datasets for training multiple models.\n",
        "\n",
        "It helps Bagging:\n",
        "\n",
        "Reduce variance and overfitting\n",
        "\n",
        "Increase stability and robustness\n",
        "\n",
        "Enable out-of-bag evaluation without a separate validation set\n",
        "\n",
        "If you want, I can draw a simple diagram showing bootstrap sampling in Bagging, which makes it very intuitive to see how different tree"
      ],
      "metadata": {
        "id": "8FfJ-vMDb4b1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What are some real-world applications of ensemble techniques?\n",
        "-  Ans- Ensemble techniques are widely used in real-world applications because they improve prediction accuracy, robustness, and generalization. Here‚Äôs a detailed overview by domain:\n",
        "\n",
        "1. Finance & Banking\n",
        "\n",
        "Credit scoring & loan approval: Predicting whether a customer will default on a loan using Random Forests or Gradient Boosting.\n",
        "\n",
        "Fraud detection: Detecting unusual transactions in real-time using ensemble classifiers (e.g., Bagging, Boosting).\n",
        "\n",
        "Stock market prediction: Combining multiple models to forecast stock prices or market trends.\n",
        "\n",
        "2. Healthcare & Medicine\n",
        "\n",
        "Disease diagnosis: Using ensemble methods to predict diseases like diabetes, cancer, or heart disease from patient data.\n",
        "\n",
        "Medical imaging: Classifying tumors in MRI or CT scans using ensemble CNNs or Random Forests.\n",
        "\n",
        "Drug discovery: Predicting chemical compound activity using boosting or stacking models.\n",
        "\n",
        "3. Marketing & Customer Analytics\n",
        "\n",
        "Customer churn prediction: Using Random Forests or Gradient Boosting to identify customers likely to leave a service.\n",
        "\n",
        "Recommendation systems: Combining multiple models to improve movie, product, or content recommendations.\n",
        "\n",
        "Targeted advertising: Ensemble models can predict which users are most likely to click on ads.\n",
        "\n",
        "4. Natural Language Processing (NLP)\n",
        "\n",
        "Spam detection: Classifying emails as spam or not using Bagging or Boosting ensembles.\n",
        "\n",
        "Sentiment analysis: Combining different classifiers to predict sentiment in text (positive/negative/neutral).\n",
        "\n",
        "Language translation or question-answering: Ensemble of multiple NLP models improves accuracy.\n",
        "\n",
        "5. Computer Vision\n",
        "\n",
        "Object detection: Combining multiple CNN models for better accuracy in detecting objects in images.\n",
        "\n",
        "Facial recognition: Ensembles improve robustness under varying lighting or angles.\n",
        "\n",
        "Self-driving cars: Multiple models are used to make safer decisions for obstacle detection and path planning.\n",
        "\n",
        "6. Cybersecurity\n",
        "\n",
        "Intrusion detection: Detecting malware, phishing attacks, or network intrusions using ensemble classifiers.\n",
        "\n",
        "Anomaly detection: Identifying unusual patterns in server logs or network traffic using Bagging or Random Forests.\n",
        "\n",
        "7. Weather & Environmental Science\n",
        "\n",
        "Weather forecasting: Combining multiple predictive models to improve short-term and long-term weather predictions.\n",
        "\n",
        "Flood prediction / climate modeling: Ensemble methods reduce uncertainty in environmental simulations.\n",
        "\n",
        "8. General Advantages in Applications\n",
        "\n",
        "Better accuracy ‚Üí important in high-stakes decisions (finance, healthcare).\n",
        "\n",
        "Robustness to noise ‚Üí improves reliability in real-world messy data.\n",
        "\n",
        "Handles complex relationships ‚Üí useful for nonlinear, high-dimensional datasets."
      ],
      "metadata": {
        "id": "9TNG7yWIdxmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is the difference between Bagging and Boosting?\n",
        "-  Ans- The main difference between Bagging and Boosting lies in how the models are trained and how predictions are combined. Both are ensemble methods, but they tackle errors differently. Here‚Äôs a detailed comparison:\n",
        "\n",
        "1. Definition\n",
        "Method\tDefinition\n",
        "Bagging (Bootstrap Aggregating)\tTrains multiple base models independently on random subsets of the training data (with replacement) and aggregates their predictions (majority vote or average).\n",
        "Boosting\tTrains base models sequentially, where each new model focuses on correcting errors made by the previous models. Predictions are combined using weighted voting or summation.\n",
        "2. Training Approach\n",
        "Feature\tBagging\tBoosting\n",
        "Sequence\tParallel (all models trained independently)\tSequential (each model depends on previous ones)\n",
        "Focus on Errors\tNo special focus on misclassified samples\tGives more weight to misclassified samples so the next model improves\n",
        "Data Sampling\tBootstrap sampling (random subsets with replacement)\tUsually uses all data but reweights samples based on previous errors\n",
        "3. Goal / Problem Addressed\n",
        "\n",
        "Bagging: Reduces variance ‚Üí stabilizes predictions, prevents overfitting (good for high-variance models like deep decision trees).\n",
        "\n",
        "Boosting: Reduces bias ‚Üí improves weak learners, reduces underfitting (good for weak base models).\n",
        "\n",
        "4. Aggregation Method\n",
        "Method\tAggregation\n",
        "Bagging\tMajority voting (classification) or averaging (regression)\n",
        "Boosting\tWeighted vote (classification) or weighted sum (regression), emphasizing models that perform better\n",
        "5. Examples\n",
        "Method\tCommon Algorithms\n",
        "Bagging\tRandom Forest, Bagged Decision Trees\n",
        "Boosting\tAdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost\n",
        "6. Overfitting\n",
        "\n",
        "Bagging reduces overfitting because averaging multiple high-variance models cancels out errors.\n",
        "\n",
        "Boosting can overfit if too many learners are used or learning rate is too high, but careful tuning usually prevents it.\n",
        "\n",
        "7. Intuition\n",
        "\n",
        "Bagging: ‚ÄúAsk 10 independent people and take a majority vote‚Äù ‚Üí reduces variance.\n",
        "\n",
        "Boosting: ‚ÄúAsk 1 person, then ask the next person to focus on where the first was wrong, and so on‚Äù ‚Üí reduces bias."
      ],
      "metadata": {
        "id": "e28o4jkWeHWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRACTICAL QUESTION"
      ],
      "metadata": {
        "id": "Vj3Hr42bei1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy?"
      ],
      "metadata": {
        "id": "TZ4YWbp-erKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Bagging Classifier with Decision Trees\n",
        "bag_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),  # Base model\n",
        "    n_estimators=50,                          # Number of trees\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bag_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "jQVEd-COe2Uy",
        "outputId": "98a0470b-dd32-436f-b8c3-c64a845211bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3173662742.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Initialize Bagging Classifier with Decision Trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m bag_clf = BaggingClassifier(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m                          \u001b[0;31m# Number of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "8AYZUeNlfXOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Bagging Regressor with Decision Trees\n",
        "bag_reg = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),  # Base model\n",
        "    n_estimators=50,                         # Number of trees\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the Bagging Regressor\n",
        "bag_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bag_reg.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Bagging Regressor Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1jpOJ3kRfTvR",
        "outputId": "27371911-71c0-4a5d-dbc4-d26a03c67cb0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaggingRegressor.__init__() got an unexpected keyword argument 'base_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3564915096.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Initialize Bagging Regressor with Decision Trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m bag_reg = BaggingRegressor(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0;31m# Number of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaggingRegressor.__init__() got an unexpected keyword argument 'base_estimator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCjwZNVrffto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_pJcRcgGetVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aFrevmpqelxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qWfdl8C7ee6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7tLtkv0ceEQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fyM6R2Qrdw5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ASHj7J3pb3sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N4TyTWYvbsZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vne8kf9bUZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UpLEaAS7bFFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SlGcZafFa0nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bEWRJae0amXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V--SYXc2aWfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jsKkDLuQaLJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zox3tIxlZ-f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RQmsR4W9ZvyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nP4S6i6lZBqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sSoDXbqYYfr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3LsMP4EVYTU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSbDsHmpYChZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G2gymeIMX06T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PLJBttd1XnAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zv47bnGVXUSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oTz08Z99XClz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aNbYJf8TW6ES"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}